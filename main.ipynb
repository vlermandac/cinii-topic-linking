{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, BNode, Namespace\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD, DC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().resolve() / \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser import parse_and_extract_articles_langs_from_dirs\n",
    "import tomllib\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "config_path =  \"config.toml\"\n",
    "\n",
    "with open(config_path, \"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "rdf_dirs = [ROOT / Path(d) for d in config[\"paths\"][\"rdf_dirs\"]]\n",
    "\n",
    "cache_dir = ROOT / \"data\" / \"cache\"\n",
    "\n",
    "data_path = ROOT / \"data\" / \"cinii_topics_benchmark\" / \"article_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = pd.read_pickle(cache_dir / \"en_articles.pkl\")\n",
    "df_jp = pd.read_pickle(cache_dir / \"jp_articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(data_path)\n",
    "\n",
    "data = pd.merge(df_csv, df_en, on=\"uri\", how=\"inner\")\n",
    "\n",
    "data = data[data['confidence'] != 'skip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of characters: 982.59\n",
      "Average number of words: 146.67\n"
     ]
    }
   ],
   "source": [
    "avg_char_length = data['abstract'].apply(len).mean()\n",
    "avg_word_length = data['abstract'].apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "print(f\"Average number of characters: {avg_char_length:.2f}\")\n",
    "print(f\"Average number of words: {avg_word_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexes import HNSWWrapper\n",
    "from indexes import build_minhash, build_lshforest_index\n",
    "\n",
    "from evaluation import (\n",
    "    precision_at_k, recall_at_k, average_precision_at_k, ndcg_at_k, f1_at_k,  # f1 optional\n",
    ")\n",
    "\n",
    "assert {\"uri\", \"title\", \"abstract\"}.issubset(data.columns), \"data must have uri/title/abstract\"\n",
    "assert data[\"uri\"].is_unique, \"uri must be unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = (data[\"title\"].fillna(\"\") + \" \" + data[\"abstract\"].fillna(\"\")).tolist()\n",
    "uris  = data[\"uri\"].astype(str).tolist()\n",
    "\n",
    "hnsw = HNSWWrapper(\n",
    "    texts=texts,\n",
    "    uris=uris,\n",
    "    emb_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    use_cosine=True,\n",
    "    ef_search=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tokens_path = ROOT / \"data\" / \"tokens\"\n",
    "word_tokens_path = tokens_path / \"words\" / \"en_words.json\"\n",
    "shingle_tokens_path = tokens_path / \"k_shingles\" / \"en_k_shingles.json\"\n",
    "\n",
    "with open(word_tokens_path, \"r\") as f:\n",
    "    word_tokens = json.load(f)\n",
    "\n",
    "with open(shingle_tokens_path, \"r\") as f:\n",
    "    shingle_tokens = json.load(f)\n",
    "\n",
    "valid_uris = set(data[\"uri\"])\n",
    "\n",
    "word_tokens = {k: v for k, v in word_tokens.items() if k in valid_uris}\n",
    "shingle_tokens = {k: v for k, v in shingle_tokens.items() if k in valid_uris}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using k-shingle tokens for LSH\n"
     ]
    }
   ],
   "source": [
    "if config[\"lsh\"][\"token_type\"] == \"words\":\n",
    "    tokens = word_tokens\n",
    "    print(\"Using word tokens for LSH\")\n",
    "\n",
    "elif config[\"lsh\"][\"token_type\"] == \"k_shingles\":\n",
    "    tokens = shingle_tokens\n",
    "    print(\"Using k-shingle tokens for LSH\")\n",
    "\n",
    "from datasketch import MinHash\n",
    "\n",
    "mh_by_uri: dict[str, MinHash] = {}\n",
    "for u, toks in tokens.items():\n",
    "    if not isinstance(toks, list):\n",
    "        raise TypeError(\"Each df1['tokens'] element must be a list[str]\")\n",
    "    mh_by_uri[u] = build_minhash([str(t) for t in toks], num_perm=config[\"lsh\"][\"num_perm\"])\n",
    "\n",
    "forest = build_lshforest_index(mh_by_uri, num_perm=config[\"lsh\"][\"num_perm\"], l=config[\"lsh\"][\"forest_l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P@5': 0.3272727272727273,\n",
       " 'R@5': 0.22972027972027972,\n",
       " 'mAP@5': 0.5344696969696969,\n",
       " 'nDCG@5': 0.36614279465027433,\n",
       " 'Queries': 143}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col = \"subtopic\"  # or \"subtopic\"\n",
    "k = 5\n",
    "exclude_self = True\n",
    "\n",
    "label_by_uri = dict(zip(data[\"uri\"].astype(str), data[label_col].astype(str)))\n",
    "\n",
    "def gold_set(query_uri: str) -> set[str]:\n",
    "    lbl = label_by_uri[query_uri]\n",
    "    return {u for u in uris if u != query_uri and label_by_uri[u] == lbl}\n",
    "\n",
    "# HNSW IR \n",
    "p_list, r_list, ap_list, nd_list = [], [], [], []\n",
    "for q in uris:\n",
    "    gold = gold_set(q)\n",
    "    if not gold:\n",
    "        continue  # skip queries with no relevant items in gold\n",
    "\n",
    "    raw = hnsw.query_by_uri(q, topk=k + (1 if exclude_self else 0), return_scores=False, exclude_self=False)\n",
    "    ranked = [u for u in raw if (not exclude_self or u != q)][:k]\n",
    "\n",
    "    rel = [1 if u in gold else 0 for u in ranked]\n",
    "    num_rel = len(gold)\n",
    "\n",
    "    p_list.append(precision_at_k(rel, k))\n",
    "    r_list.append(recall_at_k(rel, k, num_rel))\n",
    "    ap_list.append(average_precision_at_k(rel, k))\n",
    "    nd_list.append(ndcg_at_k(rel, k, num_rel))\n",
    "\n",
    "hnsw_results = {\n",
    "    f\"P@{k}\": sum(p_list)/len(p_list) if p_list else float(\"nan\"),\n",
    "    f\"R@{k}\": sum(r_list)/len(r_list) if r_list else float(\"nan\"),\n",
    "    f\"mAP@{k}\": sum(ap_list)/len(ap_list) if ap_list else float(\"nan\"),\n",
    "    f\"nDCG@{k}\": sum(nd_list)/len(nd_list) if nd_list else float(\"nan\"),\n",
    "    \"Queries\": len(p_list),\n",
    "}\n",
    "hnsw_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P@5': 0.07552447552447553,\n",
       " 'R@5': 0.04543865225683408,\n",
       " 'mAP@5': 0.15297202797202797,\n",
       " 'nDCG@5': 0.07519038691928903,\n",
       " 'Queries': 143}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSH Forest IR \n",
    "p_list, r_list, ap_list, nd_list = [], [], [], []\n",
    "for q in uris:\n",
    "    gold = gold_set(q)\n",
    "    if not gold:\n",
    "        continue\n",
    "\n",
    "    cand = forest.query(mh_by_uri[q], k + (1 if exclude_self else 0))\n",
    "    ranked = [u for u in cand if (not exclude_self or u != q)][:k]\n",
    "\n",
    "    rel = [1 if u in gold else 0 for u in ranked]\n",
    "    num_rel = len(gold)\n",
    "\n",
    "    p_list.append(precision_at_k(rel, k))\n",
    "    r_list.append(recall_at_k(rel, k, num_rel))\n",
    "    ap_list.append(average_precision_at_k(rel, k))\n",
    "    nd_list.append(ndcg_at_k(rel, k, num_rel))\n",
    "\n",
    "lsh_results = {\n",
    "    f\"P@{k}\": sum(p_list)/len(p_list) if p_list else float(\"nan\"),\n",
    "    f\"R@{k}\": sum(r_list)/len(r_list) if r_list else float(\"nan\"),\n",
    "    f\"mAP@{k}\": sum(ap_list)/len(ap_list) if ap_list else float(\"nan\"),\n",
    "    f\"nDCG@{k}\": sum(nd_list)/len(nd_list) if nd_list else float(\"nan\"),\n",
    "    \"Queries\": len(p_list),\n",
    "}\n",
    "lsh_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>P@5</th>\n",
       "      <th>R@5</th>\n",
       "      <th>mAP@5</th>\n",
       "      <th>nDCG@5</th>\n",
       "      <th>Queries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HNSW</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.110509</td>\n",
       "      <td>0.716130</td>\n",
       "      <td>0.575544</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MinHash+LSHForest</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.040739</td>\n",
       "      <td>0.452343</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Method    P@5       R@5     mAP@5    nDCG@5  Queries\n",
       "0               HNSW  0.560  0.110509  0.716130  0.575544      150\n",
       "1  MinHash+LSHForest  0.332  0.040739  0.452343  0.324514      150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    [\n",
    "        {\"Method\": \"HNSW\", **hnsw_results},\n",
    "        {\"Method\": \"MinHash+LSHForest\", **lsh_results},\n",
    "    ]\n",
    ")[[\"Method\", f\"P@{k}\", f\"R@{k}\", f\"mAP@{k}\", f\"nDCG@{k}\", \"Queries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOS_SUBTOPICS = [\n",
    "    \"Mathematics\",\n",
    "    \"Computer and Information Sciences\",\n",
    "    \"Physical Sciences\",\n",
    "    \"Chemical Sciences\",\n",
    "    \"Earth and Related Environmental Sciences\",\n",
    "    \"Biological Sciences\",\n",
    "    \"Other Natural Sciences\",\n",
    "    \"Civil Engineering\",\n",
    "    \"Electrical Engineering, Electronic Engineering, Information Engineering\",\n",
    "    \"Mechanical Engineering\",\n",
    "    \"Chemical Engineering\",\n",
    "    \"Materials Engineering\",\n",
    "    \"Medical Engineering\",\n",
    "    \"Environmental Engineering\",\n",
    "    \"Environmental Biotechnology\",\n",
    "    \"Industrial Biotechnology\",\n",
    "    \"Nanotechnology\",\n",
    "    \"Other Engineering and Technologies\",\n",
    "    \"Basic Medicine\",\n",
    "    \"Clinical Medicine\",\n",
    "    \"Health Sciences\",\n",
    "    \"Medical Biotechnology\",\n",
    "    \"Other Medical Sciences\",\n",
    "    \"Agriculture, Forestry, and Fisheries\",\n",
    "    \"Animal and Dairy Science\",\n",
    "    \"Veterinary Science\",\n",
    "    \"Agricultural Biotechnology\",\n",
    "    \"Other Agricultural Sciences\",\n",
    "    \"Psychology\",\n",
    "    \"Economics and Business\",\n",
    "    \"Educational Sciences\",\n",
    "    \"Sociology\",\n",
    "    \"Law\",\n",
    "    \"Political Science\",\n",
    "    \"Social and Economic Geography\",\n",
    "    \"Media and Communications\",\n",
    "    \"Other Social Sciences\",\n",
    "    \"History and Archaeology\",\n",
    "    \"Languages and Literature\",\n",
    "    \"Philosophy, Ethics and Religion\",\n",
    "    \"Art (Arts, History of Arts, Performing Arts, Music)\",\n",
    "    \"Other Humanities\",\n",
    "]\n",
    "FOS_MAJOR_TOPICS = [\n",
    "    \"Natural Sciences\",\n",
    "    \"Engineering and Technology\",\n",
    "    \"Medical and Health Sciences\",\n",
    "    \"Agricultural Sciences\",\n",
    "    \"Social Sciences\",\n",
    "    \"Humanities\",\n",
    "]\n",
    "ALLOWED_TOPICS = FOS_SUBTOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, create_model\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# columns: ['uri', 'title', 'abstract', 'subtopic']\n",
    "assert {\"title\", \"abstract\", \"subtopic\"}.issubset(data.columns)\n",
    "\n",
    "# Dynamic Pydantic model with a Literal enum over topics\n",
    "# (All fields required, additionalProperties=false under the hood)\n",
    "TopicPred = create_model(\n",
    "    \"TopicPred\",\n",
    "    predicted_subtopic=(Literal[tuple(ALLOWED_TOPICS)], ...)  # required\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-mini\")\n",
    "\n",
    "SYSTEM = \"You classify scholarly records into ONE OECD FOS topic from the allowed set. Output must follow the schema.\"\n",
    "USER_TPL = \"\"\"Choose ONE subtopic from the allowed list.\n",
    "\n",
    "ALLOWED SUBTOPICS:\n",
    "{allowed}\n",
    "\n",
    "RECORD\n",
    "Title: {title}\n",
    "Abstract: {abstract}\n",
    "\"\"\"\n",
    "\n",
    "def predict_one(title: str, abstract: str) -> str:\n",
    "    user = USER_TPL.format(\n",
    "        allowed=\"\\n- \" + \"\\n- \".join(ALLOWED_TOPICS),\n",
    "        title=(title or \"\").strip(),\n",
    "        abstract=(abstract or \"\").strip()\n",
    "    )\n",
    "\n",
    "    resp = client.responses.parse(\n",
    "        model=MODEL,\n",
    "        input=[{\"role\": \"system\", \"content\": SYSTEM},\n",
    "               {\"role\": \"user\", \"content\": user}],\n",
    "        text_format=TopicPred,         # <- Structured Outputs via Pydantic\n",
    "    )\n",
    "\n",
    "    # resp.output_parsed is a TopicPred instance\n",
    "    return resp.output_parsed.predicted_subtopic\n",
    "\n",
    "# Run predictions\n",
    "preds = []\n",
    "for _, row in data.iterrows():\n",
    "    preds.append(predict_one(row[\"title\"], row[\"abstract\"]))\n",
    "    time.sleep(0.1)  # pacing if needed\n",
    "\n",
    "data_eval = data.copy()\n",
    "data_eval[\"pred_subtopic\"] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.6467\n",
      "Macro Precision:  0.4933\n",
      "Macro Recall:     0.5174\n",
      "Macro F1:         0.4788\n",
      "\n",
      "Per-class (head):\n",
      "                                                                         precision    recall  f1-score   support\n",
      "\n",
      "                                             Agricultural Biotechnology       0.00      0.00      0.00         1\n",
      "                                   Agriculture, Forestry, and Fisheries       0.80      0.50      0.62         8\n",
      "                                               Animal and Dairy Science       0.33      1.00      0.50         1\n",
      "                                                         Basic Medicine       0.60      0.50      0.55         6\n",
      "                                                    Biological Sciences       0.64      0.75      0.69        12\n",
      "                                                   Chemical Engineering       0.00      0.00      0.00         3\n",
      "                                                      Chemical Sciences       0.67      0.44      0.53         9\n",
      "                                                      Civil Engineering       0.75      0.75      0.75         4\n",
      "                                                      Clinical Medicine       1.00      0.86      0.92         7\n",
      "                                      Computer and Information Sciences       0.83      0.77      0.80        13\n",
      "                               Earth and Related Environmental Sciences       0.33      0.33      0.33         6\n",
      "                                                 Economics and Business       0.50      1.00      0.67         1\n",
      "                                                   Educational Sciences       0.75      0.75      0.75         4\n",
      "Electrical Engineering, Electronic Engineering, Information Engineering       0.42      0.62      0.50         8\n",
      "                                            Environmental Biotechnology       0.00      0.00      0.00         0\n",
      "                                              Environmental Engineering       0.29      0.67      0.40         3\n",
      "                                                        Health Sciences       \n"
     ]
    }
   ],
   "source": [
    "gold = data_eval[\"subtopic\"].astype(str).tolist() # major_topic or subtopic\n",
    "pred = data_eval[\"pred_subtopic\"].astype(str).tolist()\n",
    "\n",
    "acc = accuracy_score(gold, pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(gold, pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(f\"Accuracy:         {acc:.4f}\")\n",
    "print(f\"Macro Precision:  {prec:.4f}\")\n",
    "print(f\"Macro Recall:     {rec:.4f}\")\n",
    "print(f\"Macro F1:         {f1:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class (head):\")\n",
    "print(classification_report(gold, pred, zero_division=0)[:2000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "internship"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
