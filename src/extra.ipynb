{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from indexes import build_minhash, build_lsh_index  # your helpers\n",
    "\n",
    "def ensure_uri(df: pd.DataFrame, lang_code: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"uri\" not in out.columns:\n",
    "        out[\"uri\"] = [f\"{lang_code}_{i}\" for i in range(len(out))]\n",
    "    out[\"lang\"] = lang_code\n",
    "    return out[[\"uri\", \"title\", \"abstract\", \"lang\"]]\n",
    "\n",
    "df_en2 = ensure_uri(df_en, \"en\")\n",
    "df_jp2 = ensure_uri(df_jp, \"ja\")\n",
    "\n",
    "# df_all = pd.concat([df_en2, df_jp2], ignore_index=True)\n",
    "# df_all = df_en2.copy()  # only English for now\n",
    "df_all = df_jp2.copy()  # only Japanese for now\n",
    "\n",
    "title_clean = df_all[\"title\"].fillna(\"\").astype(str).str.strip()\n",
    "abs_clean   = df_all[\"abstract\"].fillna(\"\").astype(str).str.strip()\n",
    "mask_nonempty = ~(title_clean.eq(\"\") & abs_clean.eq(\"\"))\n",
    "df_all = df_all.loc[mask_nonempty].reset_index(drop=True)\n",
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2addec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows whose abstract contains the exact substring\n",
    "before = len(df_all)\n",
    "mask = df_all[\"abstract\"].fillna(\"\").str.contains(\"identifier:\", regex=False)\n",
    "df_all = df_all.loc[~mask].reset_index(drop=True)\n",
    "removed = int(mask.sum())\n",
    "print(f\"Removed {removed} rows (before: {before}, after: {len(df_all)})\")\n",
    "# remove rows whose abstract contains the exact substring '...' and just that\n",
    "before = len(df_all)\n",
    "mask = df_all[\"abstract\"].fillna(\"\").str.contains(r\"^...\", regex=True)\n",
    "df_all = df_all.loc[~mask].reset_index(drop=True)\n",
    "removed = int(mask.sum())\n",
    "print(f\"Removed {removed} rows (before: {before}, after: {len(df_all)})\")\n",
    "\n",
    "pattern = r\"^\\s*(\\.{3}|…|‥|…{2}|。{3})\\s*$\"\n",
    "before = len(df_all)\n",
    "mask = df_all[\"abstract\"].fillna(\"\").str.match(pattern)\n",
    "df_all = df_all.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "removed = int(mask.sum())\n",
    "print(f\"Removed {removed} rows (before: {before}, after: {len(df_all)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc857d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHINGLE_NS   = (2, 3)\n",
    "NUM_PERM     = 128\n",
    "LSH_THRESHOLD = 0.3  # tune 0.5–0.8 for more/less bucket collisions\n",
    "\n",
    "word_re = re.compile(r\"\\w+\", re.UNICODE)\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return word_re.findall(str(text).lower()) if isinstance(text, str) else []\n",
    "\n",
    "def make_shingles(tokens: list[str], ns=SHINGLE_NS) -> set[str]:\n",
    "    S = set()\n",
    "    for n in ns:\n",
    "        if len(tokens) >= n:\n",
    "            S.update(\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "    return S\n",
    "\n",
    "texts = (df_all[\"title\"].fillna(\"\") + \" \" + df_all[\"abstract\"].fillna(\"\")).tolist()\n",
    "uris  = df_all[\"uri\"].tolist()\n",
    "\n",
    "minhash_dict = {}\n",
    "for u, txt in zip(uris, texts):\n",
    "    toks = tokenize(txt)\n",
    "    sh   = make_shingles(toks)\n",
    "    mh   = build_minhash(sorted(sh), num_perm=NUM_PERM)\n",
    "    minhash_dict[u] = mh\n",
    "\n",
    "lsh = build_lsh_index(minhash_dict, threshold=LSH_THRESHOLD, num_perm=NUM_PERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh_unique_buckets(lsh, min_size: int = 3, top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Return the largest UNIQUE (band, bucket) groups from a datasketch.MinHashLSH.\n",
    "    A group = all keys that collide in the same (band, bucket).\n",
    "    If the exact same member set appears in multiple bands, it is returned once.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lsh : datasketch.MinHashLSH\n",
    "        An already-built LSH index.\n",
    "    min_size : int\n",
    "        Minimum number of members required to keep a bucket group.\n",
    "    top_n : int\n",
    "        Return only the top-N largest unique groups (use None/0 for all).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[dict]\n",
    "        Each dict has: {\"band\", \"bucket\", \"size\", \"members\"} with members as a list of keys.\n",
    "    \"\"\"\n",
    "    # 1) Per-band bucket counts\n",
    "    counts_per_band = lsh.get_counts()  # list[dict[bucket_key -> count]]\n",
    "\n",
    "    unique = {}  # signature (tuple(sorted members)) -> row dict\n",
    "    for band_idx, counts in enumerate(counts_per_band):\n",
    "        ht = lsh.hashtables[band_idx]\n",
    "        for bucket_key, c in counts.items():\n",
    "            if c < min_size:\n",
    "                continue\n",
    "            # 2) Get members for this (band, bucket)\n",
    "            members_raw = ht.get(bucket_key)  # iterable of keys\n",
    "            # Canonicalize members to strings and sort to form a dedup signature\n",
    "            members = [\n",
    "                (m.decode(\"utf-8\", \"ignore\") if isinstance(m, bytes) else str(m))\n",
    "                for m in members_raw\n",
    "            ]\n",
    "            members_sorted = tuple(sorted(members))\n",
    "            if len(members_sorted) < min_size:\n",
    "                continue\n",
    "\n",
    "            # 3) Deduplicate identical member sets across bands\n",
    "            if members_sorted not in unique:\n",
    "                unique[members_sorted] = {\n",
    "                    \"band\": band_idx,\n",
    "                    \"bucket\": bucket_key,\n",
    "                    \"size\": len(members_sorted),\n",
    "                    \"members\": list(members_sorted),\n",
    "                }\n",
    "            else:\n",
    "                # (optional) keep the largest occurrence; here sets are identical so size is same\n",
    "                pass\n",
    "\n",
    "    rows = sorted(unique.values(), key=lambda r: r[\"size\"], reverse=True)\n",
    "    return rows[:top_n] if top_n else rows\n",
    "\n",
    "bucket_list = lsh_unique_buckets(lsh, min_size=3)\n",
    "print(f\"Found {len(bucket_list)} buckets with >=3 members\")\n",
    "# print the sizes of the top 5 largest buckets (just the sizes)\n",
    "for i, b in enumerate(bucket_list[:5]):\n",
    "    print(f\"Bucket {i+1}: size {len(b['members'])}\")\n",
    "\n",
    "# print the members of the bucket 5 and their titles + abstracts\n",
    "bucket5 = bucket_list[7]  # 0-based index\n",
    "print(f\"\\nMembers of bucket 5 (size {len(bucket5['members'])}):\")\n",
    "for uri in bucket5[\"members\"]:\n",
    "    title = df_all.loc[df_all[\"uri\"] == uri, \"title\"].values[0]\n",
    "    abstract = df_all.loc[df_all[\"uri\"] == uri, \"abstract\"].values[0]\n",
    "    print(f\"- URI: {uri}\\n  Title: {title}\\n  Abstract: {abstract[:100]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import extract_keywords_df\n",
    "\n",
    "# Extract keywords from the DataFrame\n",
    "# df_en_kw, topics = extract_keywords_df(\n",
    "#     df_en,\n",
    "#     text_cols=(\"title\", \"abstract\"),\n",
    "#     top_k=8,\n",
    "#     ngram_range=(1, 3),\n",
    "#     stop_words=\"english\",\n",
    "#     keywords_col=\"keywords\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "id = 20\n",
    "q = data[\"uri\"].astype(str).iloc[id]\n",
    "q_title = data.loc[data[\"uri\"] == q, \"title\"].values[0]\n",
    "print(f\"Querying for URI: {q}\\nTitle: {q_title}\\n\")\n",
    "\n",
    "# HNSW neighbors (URIs), self excluded here\n",
    "hnsw_neighbors = hnsw.query_by_uri(q, topk=k, return_scores=False, exclude_self=True)\n",
    "\n",
    "# LSH Forest neighbors (ask for k+1, drop self if present)\n",
    "lsh_neighbors = forest.query(mh_by_uri[q], k + 1)\n",
    "# lsh_neighbors = [u for u in lsh_neighbors if u != q][:k]\n",
    "\n",
    "# print the top 5 neighbors titles and abstracts\n",
    "print(\"HNSW Neighbors:\")\n",
    "for uri in hnsw_neighbors[:5]:\n",
    "    title = data.loc[data[\"uri\"] == uri, \"title\"].values[0]\n",
    "    abstract = data.loc[data[\"uri\"] == uri, \"abstract\"].values[0]\n",
    "    print(f\"Title: {title}\\nAbstract: {abstract}\\n\")\n",
    "\n",
    "print(\"LSH Neighbors:\")\n",
    "for uri in lsh_neighbors[:5]:\n",
    "    title = data.loc[data[\"uri\"] == uri, \"title\"].values[0]\n",
    "    abstract = data.loc[data[\"uri\"] == uri, \"abstract\"].values[0]\n",
    "    print(f\"Title: {title}\\nAbstract: {abstract}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
